{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"[E1041] Expected a string, Doc, or bytes as input, but got: <class '_io.TextIOWrapper'>","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcody1\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOneDrive\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mITEC-4230\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mKaggle-2024\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOfficial\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(entity\u001b[38;5;241m.\u001b[39mlabel_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m entity\u001b[38;5;241m.\u001b[39mtext)\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\language.py:1037\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1018\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1022\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1037\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1039\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\language.py:1131\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[1;34m(self, doc_like)\u001b[0m\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n\u001b[1;32m-> 1131\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE1041\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(doc_like)))\n","\u001b[1;31mValueError\u001b[0m: [E1041] Expected a string, Doc, or bytes as input, but got: <class '_io.TextIOWrapper'>"]}],"source":["import spacy\n","\n","text = open(r\"C:\\Users\\cody1\\OneDrive\\Desktop\\ITEC-4230\\Kaggle-2024\\datasets\\Official\\test.json\")\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","doc = nlp(text)\n","\n","for entity in doc.ents:\n","    print(entity.label_ +\": \"+ entity.text)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        10\n","\n","    accuracy                           1.00        10\n","   macro avg       1.00      1.00      1.00        10\n","weighted avg       1.00      1.00      1.00        10\n","\n","Predicted label: Non-PII\n"]}],"source":["import json\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","\n","# Load data\n","with open('../datasets/Official/train.json', 'r') as f:\n","    train_data = json.load(f)\n","\n","with open('../datasets/Official/test.json', 'r') as f:\n","    test_data = json.load(f)\n","\n","# Extract features and labels\n","X_train = [' '.join(doc['tokens']) for doc in train_data]\n","y_train = [1 if 'B-' in doc.get('labels', []) else 0 for doc in train_data]  # 1 for PII, 0 for non-PII\n","\n","X_test = [' '.join(doc['tokens']) for doc in test_data]\n","y_test = [1 if 'B-' in doc.get('labels', []) else 0 for doc in test_data]  # 1 for PII, 0 for non-PII\n","\n","# Feature extraction using Bag of Words\n","vectorizer = CountVectorizer()\n","X_train_bow = vectorizer.fit_transform(X_train)\n","X_test_bow = vectorizer.transform(X_test)\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train_bow.toarray(), dtype=torch.float32)\n","y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n","X_test_torch = torch.tensor(X_test_bow.toarray(), dtype=torch.float32)\n","y_test_torch = torch.tensor(y_test, dtype=torch.float32)\n","\n","# Define the Random Forest model using PyTorch\n","class RandomForest(nn.Module):\n","    def __init__(self, input_size):\n","        super(RandomForest, self).__init__()\n","        self.input_size = input_size\n","        self.fc = nn.Linear(input_size, 1)\n","\n","    def forward(self, x):\n","        return torch.sigmoid(self.fc(x))\n","\n","# Train the Random Forest model\n","model = RandomForest(input_size=X_train_torch.shape[1])\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","num_epochs = 10\n","batch_size = 64\n","\n","for epoch in range(num_epochs):\n","    for i in range(0, len(X_train_torch), batch_size):\n","        inputs = X_train_torch[i:i+batch_size]\n","        targets = y_train_torch[i:i+batch_size]\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets.view(-1, 1))\n","        loss.backward()\n","        optimizer.step()\n","\n","# Evaluate the model\n","with torch.no_grad():\n","    model.eval()\n","    outputs = model(X_test_torch)\n","    predicted_labels = (outputs >= 0.5).squeeze().cpu().numpy()\n","    print(classification_report(y_test, predicted_labels))\n","\n","# Example prediction\n","example_text = \"John Doe's email is john.doe@example.com\"\n","example_features = vectorizer.transform([example_text])\n","example_features_torch = torch.tensor(example_features.toarray(), dtype=torch.float32)\n","with torch.no_grad():\n","    model.eval()\n","    output = model(example_features_torch)\n","    predicted_label = (output >= 0.5).squeeze().cpu().numpy()\n","    print(\"Predicted label:\", \"PII\" if predicted_label == 1 else \"Non-PII\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["         row_id  document       token label\n","0             0         7      Design     O\n","1             1         7    Thinking     O\n","2             2         7         for     O\n","3             3         7  innovation     O\n","4             4         7   reflexion     O\n","...         ...       ...         ...   ...\n","4992528     815     22687     process     O\n","4992529     816     22687   explained     O\n","4992530     817     22687       above     O\n","4992531     818     22687           .     O\n","4992532     819     22687        \\n\\n     O\n","\n","[4992533 rows x 4 columns]\n"]}],"source":["import json\n","import pandas as pd\n","\n","# Load data\n","with open('../datasets/Official/train.json', 'r') as f:\n","    train_data = json.load(f)\n","\n","# Initialize lists to store extracted data\n","output_data = []\n","\n","# Iterate through each document in the training data\n","for doc in train_data:\n","    document_id = doc['document']\n","    full_text = doc['full_text']\n","    tokens = doc['tokens']\n","    labels = doc.get('labels', [])\n","\n","    # Iterate through tokens and labels\n","    for i, (token, label) in enumerate(zip(tokens, labels)):\n","        output_data.append({\n","            'row_id': i,\n","            'document': document_id,\n","            'token': token,\n","            'label': label\n","        })\n","\n","# Convert output data to DataFrame\n","output_df = pd.DataFrame(output_data)\n","\n","# Display the DataFrame\n","print(output_df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"MemoryError","evalue":"Unable to allocate 331. GiB for an array with shape (4992533,) and data type <U17820","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)","Cell \u001b[1;32mIn[6], line 42\u001b[0m\n\u001b[0;32m     34\u001b[0m         output_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     35\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow_id\u001b[39m\u001b[38;5;124m'\u001b[39m: i,\n\u001b[0;32m     36\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocument\u001b[39m\u001b[38;5;124m'\u001b[39m: document_id,\n\u001b[0;32m     37\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m: token,\n\u001b[0;32m     38\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: label\n\u001b[0;32m     39\u001b[0m         })\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Convert lists to NumPy arrays\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m y_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_train)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Feature extraction using Bag of Words\u001b[39;00m\n","\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 331. GiB for an array with shape (4992533,) and data type <U17820"]}],"source":["import json\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import classification_report\n","\n","# Load data\n","with open('../datasets/Official/train.json', 'r') as f:\n","    train_data = json.load(f)\n","\n","# Initialize lists to store extracted data\n","output_data = []\n","\n","# Extract features and labels\n","X_train = []\n","y_train = []\n","\n","# Iterate through each document in the training data\n","for doc in train_data:\n","    document_id = doc['document']\n","    full_text = doc['full_text']\n","    tokens = doc['tokens']\n","    labels = doc.get('labels', [])\n","\n","    # Iterate through tokens and labels\n","    for i, (token, label) in enumerate(zip(tokens, labels)):\n","        X_train.append(' '.join(tokens[:i+1]))  # Use tokens up to the current position\n","        y_train.append(1 if 'B-' in label else 0)  # 1 for PII, 0 for non-PII\n","\n","        # Store extracted data\n","        output_data.append({\n","            'row_id': i,\n","            'document': document_id,\n","            'token': token,\n","            'label': label\n","        })\n","\n","# Convert lists to NumPy arrays\n","X_train = np.array(X_train)\n","y_train = np.array(y_train)\n","\n","# Feature extraction using Bag of Words\n","vectorizer = CountVectorizer()\n","X_train_bow = vectorizer.fit_transform(X_train)\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(X_train_bow.toarray(), dtype=torch.float32)\n","y_train_torch = torch.tensor(y_train, dtype=torch.float32)\n","\n","# Define the Random Forest model using PyTorch\n","class RandomForest(nn.Module):\n","    def __init__(self, input_size):\n","        super(RandomForest, self).__init__()\n","        self.input_size = input_size\n","        self.fc = nn.Linear(input_size, 1)\n","\n","    def forward(self, x):\n","        return torch.sigmoid(self.fc(x))\n","\n","# Train the Random Forest model\n","model = RandomForest(input_size=X_train_torch.shape[1])\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","num_epochs = 10\n","batch_size = 64\n","\n","for epoch in range(num_epochs):\n","    for i in range(0, len(X_train_torch), batch_size):\n","        inputs = X_train_torch[i:i+batch_size]\n","        targets = y_train_torch[i:i+batch_size]\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets.view(-1, 1))\n","        loss.backward()\n","        optimizer.step()\n","\n","# Extracting PII from the test data can be done similarly as done for the training data\n","# You can also evaluate the model's performance using classification_report as before\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"zero-dimensional arrays cannot be concatenated","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m     y_train_batches\u001b[38;5;241m.\u001b[39mappend(y_batch)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Convert data to PyTorch tensors\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m X_train_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_batches\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     47\u001b[0m y_train_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(np\u001b[38;5;241m.\u001b[39mconcatenate(y_train_batches), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Define the Random Forest model using PyTorch\u001b[39;00m\n","\u001b[1;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"]}],"source":["# Load data\n","with open('../datasets/Official/train.json', 'r') as f:\n","    train_data = json.load(f)\n","\n","# Extract features and labels\n","X_train = []\n","y_train = []\n","\n","# Iterate through each document in the training data\n","for doc in train_data:\n","    document_id = doc['document']\n","    tokens = doc['tokens']\n","    labels = doc.get('labels', [])\n","\n","    # Iterate through tokens and labels\n","    for i, (token, label) in enumerate(zip(tokens, labels)):\n","        X_train.append(' '.join(tokens[:i+1]))  # Use tokens up to the current position\n","        y_train.append(1 if 'B-' in label else 0)  # 1 for PII, 0 for non-PII\n","\n","# Feature extraction using Bag of Words\n","vectorizer = CountVectorizer()\n","X_train_bow = vectorizer.fit_transform(X_train)\n","\n","# Define batch size\n","batch_size = 1000\n","\n","# Define a function to process data in batches\n","def process_batch(batch_data):\n","    X_batch = [' '.join(doc['tokens']) for doc in batch_data]\n","    y_batch = [1 if 'B-' in doc.get('labels', []) else 0 for doc in batch_data]\n","    X_batch_bow = vectorizer.transform(X_batch)\n","    return X_batch_bow, np.array(y_batch)\n","\n","# Process data in batches\n","X_train_batches = []\n","y_train_batches = []\n","\n","num_batches = len(train_data) // batch_size + 1\n","for i in range(num_batches):\n","    batch_data = train_data[i*batch_size:(i+1)*batch_size]\n","    X_batch, y_batch = process_batch(batch_data)\n","    X_train_batches.append(X_batch)\n","    y_train_batches.append(y_batch)\n","\n","# Convert data to PyTorch tensors\n","X_train_torch = torch.tensor(np.concatenate(X_train_batches), dtype=torch.float32)\n","y_train_torch = torch.tensor(np.concatenate(y_train_batches), dtype=torch.float32)\n","\n","# Define the Random Forest model using PyTorch\n","class RandomForest(nn.Module):\n","    def __init__(self, input_size):\n","        super(RandomForest, self).__init__()\n","        self.input_size = input_size\n","        self.fc = nn.Linear(input_size, 1)\n","\n","    def forward(self, x):\n","        return torch.sigmoid(self.fc(x))\n","\n","# Train the Random Forest model\n","model = RandomForest(input_size=X_train_torch.shape[1])\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n","        inputs = torch.tensor(X_batch.toarray(), dtype=torch.float32)\n","        targets = torch.tensor(y_batch, dtype=torch.float32)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets.view(-1, 1))\n","        loss.backward()\n","        optimizer.step()\n","\n","# Extracting PII from the test data can be done similarly as done for the training data\n","# You can also evaluate the model's performance using classification_report as before"]},{"cell_type":"markdown","metadata":{},"source":["# Pre-processing"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","import json\n","\n","# Define the path to the JSON file\n","json_file_path = '../datasets/Official/train.json'\n","\n","# Initialize an empty list to store the training data\n","training_data = []\n","\n","# Read the JSON file and store the data in the training_data list\n","with open(json_file_path, 'r') as file:\n","    training_data = json.load(file)\n","\n","# Prepare data for preprocessing\n","documents = []\n","token_sequences = []\n","labels_sequences = []\n","\n","# Iterate over each data object\n","for data in training_data:\n","    document = data[\"document\"]\n","    full_text = data[\"full_text\"]\n","    tokens = data[\"tokens\"]\n","    labels = data[\"labels\"]\n","\n","    # Join tokens into a single string\n","    token_sequence = \" \".join(tokens)\n","    \n","    # Join labels into a single string\n","    label_sequence = \" \".join(labels)\n","    \n","    # Append to lists\n","    documents.append(document)\n","    token_sequences.append(token_sequence)\n","    labels_sequences.append(label_sequence)\n","\n","# Initialize and fit CountVectorizer for token sequences\n","vectorizer = CountVectorizer()\n","X_token = vectorizer.fit_transform(token_sequences)\n","\n","# Initialize and fit LabelEncoder for labels\n","label_encoder = LabelEncoder()\n","y_label = label_encoder.fit_transform(labels_sequences)\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X_token, y_label, test_size=0.2, random_state=42)\n","\n","# You may also need to perform padding, depending on the requirements of your model and library used\n","# For example, if using PyTorch, you can pad sequences using torch.nn.utils.rnn.pad_sequence\n","# If using TensorFlow/Keras, you can use tf.keras.preprocessing.sequence.pad_sequences\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["O\n"]},{"ename":"IndexError","evalue":"string index out of range","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[1;32mIn[11], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(label_info)  \u001b[38;5;66;03m# Add this line for debugging\u001b[39;00m\n\u001b[0;32m     21\u001b[0m label_type \u001b[38;5;241m=\u001b[39m label_info[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Correct access to label type\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m   \u001b[38;5;66;03m# Correct access to start index\u001b[39;00m\n\u001b[0;32m     23\u001b[0m end_idx \u001b[38;5;241m=\u001b[39m label_info[\u001b[38;5;241m2\u001b[39m]     \u001b[38;5;66;03m# Correct access to end index\u001b[39;00m\n\u001b[0;32m     24\u001b[0m labels[start_idx:end_idx] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m label_type] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mI-\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m label_type] \u001b[38;5;241m*\u001b[39m (end_idx \u001b[38;5;241m-\u001b[39m start_idx \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n","\u001b[1;31mIndexError\u001b[0m: string index out of range"]}],"source":["import json\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report\n","\n","# Step 1: Read training data\n","with open('../datasets/Official/train.json', 'r') as file:\n","    train_data = json.load(file)\n","\n","# Step 2: Preprocess training data\n","train_texts = []\n","train_labels = []\n","\n","for item in train_data:\n","    text = item['full_text']\n","    labels = ['O'] * len(item['tokens'])  # Initialize all labels as 'O'\n","    for label_info in item['labels']:\n","        print(label_info)  # Add this line for debugging\n","        label_type = label_info[0]  # Correct access to label type\n","        start_idx = label_info[1]   # Correct access to start index\n","        end_idx = label_info[2]     # Correct access to end index\n","        labels[start_idx:end_idx] = ['B-' + label_type] + ['I-' + label_type] * (end_idx - start_idx - 1)\n","    train_texts.append(text)\n","    train_labels.extend(labels)\n","\n","# Step 3: Train machine learning model\n","vectorizer = CountVectorizer()\n","X_train = vectorizer.fit_transform(train_texts)\n","y_train = train_labels\n","\n","model = LogisticRegression(max_iter=1000)\n","model.fit(X_train, y_train)\n","\n","# Step 4: Read test data\n","with open('../datasets/Official/test.json', 'r') as file:\n","    test_data = json.load(file)\n","\n","# Step 5: Preprocess test data\n","test_texts = [item['full_text'] for item in test_data]\n","\n","# Step 6: Use the trained model to predict labels for test data\n","X_test = vectorizer.transform(test_texts)\n","predicted_labels = model.predict(X_test)\n","\n","# Step 7: Write predicted labels to output file\n","output_data = []\n","for idx, item in enumerate(test_data):\n","    for token_idx, token in enumerate(item['tokens']):\n","        output_data.append([idx, item['document'], token_idx, predicted_labels.pop(0)])\n","\n","# Write the output data to a CSV file\n","output_file = \"../datasets/output.csv\"\n","with open(output_file, \"w\") as file:\n","    # Write the header\n","    file.write(\"row_id,document,token,label\\n\")\n","    \n","    # Write each data row\n","    for row in output_data:\n","        file.write(\",\".join(map(str, row)) + \"\\n\")\n","\n","print(\"Output file created:\", output_file)\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[ 0.18423669  0.45122683 -0.80794598 -0.45071834]\n"," [ 0.05537981 -0.90087639  0.40947615 -0.96117093]\n"," [-0.85066716 -0.98671898  1.381056    1.86525244]]\n"]},{"name":"stderr","output_type":"stream","text":["C:\\Users\\cody1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n","  warnings.warn(\n","C:\\Users\\cody1\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  warnings.warn(\n"]}],"source":["from sklearn import svm\n","from sklearn import datasets\n","# Load dataset\n","iris = datasets.load_iris()\n","clf = svm.LinearSVC()\n","# learn from the data\n","clf.fit(iris.data, iris.target)\n","# predict for unseen data\n","clf.predict([[ 5.0,  3.6,  1.3,  0.25]])\n","# Parameters of model can be changed by using the attributes ending with an underscore\n","print(clf.coef_ )"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":2}
