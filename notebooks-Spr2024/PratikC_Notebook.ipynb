{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Loading and Cleaning Datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f81b8885e3011eb3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy as sp\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If using IDE, Run \n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "in the bash to install the english spacy pipline"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3a07f48d94a5870"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Official Datasets"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9086b614e399c7c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data and Exploration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc60c15cf0c78d6e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af5679c6eceac05",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"../Datasets/Official/train.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a7591efc0b31c",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter()\n",
    "df.apply(lambda line: c.update(line.labels), axis = 1)\n",
    "c_pii = c.most_common()[1:]\n",
    "c_key, c_val = zip(*c_pii)\n",
    "plt.barh(c_key, c_val)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ee442bede747650a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "54b7aa96057ad617",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c129aa100c95f41a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.05)\n",
    "dft = df_test.head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1316a275eb4d5dd6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "pattern = '(\\xa0|\\uf0b7)'\n",
    "df.loc[:,'full_text'] = df.loc[:,'full_text'].replace(pattern, ' ')\n",
    "df.loc[:,'tokens'] = df.loc[:,'tokens'].apply(lambda line: [tok for tok in line if not re.search(pattern1,tok)])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "cb746d6a26f36293",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "457bd3b4533f5a80"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sample_row = df.iloc[16]\n",
    "sample_tokens = sample_row.tokens\n",
    "sample_labels = sample_row.labels\n",
    "# sample_row\n",
    "sample_tokens\n",
    "# sample_labels"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ca31ad09622375d0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# encoding = tokenizer.encode_plus(\n",
    "#     sample_text,\n",
    "#     add_special_tokens=True,\n",
    "#     max_length=512,\n",
    "#     return_token_type_ids=False,\n",
    "#     padding=\"max_length\",\n",
    "#     return_attention_mask=True,\n",
    "#     return_tensors='pt',\n",
    "# )\n",
    "# \n",
    "# encoding[\"input_ids\"].squeeze()[:20]\n",
    "# encoding[\"attention_mask\"].squeeze()[:20]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4963900a4ec7adea",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from transformers import BertForTokenClassification\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import glob\n",
    "\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c3b500abcb9edc70",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# token_counts = []\n",
    "# for _, row in df_train.iterrows():\n",
    "#     token_count = len(tokenizer.encode(\n",
    "#         row[\"full_text\"],\n",
    "#         max_length=2048,\n",
    "#         truncation=True\n",
    "#     ))\n",
    "#     token_counts.append(token_count)\n",
    "# sns.histplot(token_counts)\n",
    "# # plt.xlim([0, 512]);"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b14634bb68d4fafe",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# platform = 'Kaggle'\n",
    "platform = 'local'\n",
    "model_name = 'model1_bert_base_cased.bin'\n",
    "\n",
    "if platform == 'Kaggle':\n",
    "    bert_path = '../input/huggingface-bert/bert-base-uncased/'\n",
    "    train_path = '/kaggle/input/coleridgeinitiative-show-us-the-data/train/'\n",
    "    test_path = '/kaggle/input/coleridgeinitiative-show-us-the-data/test/*'\n",
    "    model_path = '../input/coleridgemodels/' + model_name\n",
    "elif platform == 'local':\n",
    "    bert_path = 'bert-base-cased'\n",
    "    model_path = '../models/bert_models/' + model_name\n",
    "\n",
    "config = {\n",
    "        'MAX_LEN': 512,\n",
    "        'tokenizer': BertTokenizer.from_pretrained(bert_path),\n",
    "        'batch_size':5,\n",
    "        'Epoch': 1,\n",
    "        'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        'model_name':model_name\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "83348d21f95dadf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def make_shorter_sentence(sentence):\n",
    "#     '''\n",
    "#     This function is to split the long sentences into chunks of shorter sentences upto the \n",
    "#     maximum length of words specified in config['MAX_LEN']\n",
    "#     '''\n",
    "#     sent_tokenized = sent_tokenize(sentence)\n",
    "# \n",
    "#     max_length = config['MAX_LEN']\n",
    "#     overlap = 20\n",
    "# \n",
    "#     final_sentences = []\n",
    "# \n",
    "#     for tokenized_sent in sent_tokenized:\n",
    "#         sent_tokenized_clean = sent_tokenized_clean.replace('.','').rstrip()\n",
    "# \n",
    "#         tok_sent = sent_tokenized_clean.split(\" \")\n",
    "# \n",
    "#         if len(tok_sent)<max_length:\n",
    "#             final_sentences.append(sent_tokenized_clean)\n",
    "#         else :\n",
    "#             #             print(\"Making shorter sentences\")\n",
    "#             start = 0\n",
    "#             end = len(tok_sent)\n",
    "# \n",
    "#             for i in range(start, end, max_length-overlap):\n",
    "#                 temp = tok_sent[i: (i + max_length)]\n",
    "#                 final_sentences.append(\" \".join(i for i in temp))\n",
    "# \n",
    "#     return final_sentences"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4450a6d1a37f8bea"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def labelling(dataset, data_dict):\n",
    "#     '''\n",
    "#     This function is to iterate each of the training data and get it labelled \n",
    "#     from the form_labels() function.\n",
    "#     '''\n",
    "# \n",
    "#     Id_list_ = []\n",
    "#     sentences_ = []\n",
    "#     key_ = []\n",
    "#     labels_ = []\n",
    "#     un_mat = []\n",
    "#     un_matched_reviews = 0\n",
    "# \n",
    "#     for i, Id in tqdm(enumerate(dataset.Id), total=len(dataset.Id)):\n",
    "# \n",
    "#         sentence = data_joining(data_dict[Id])\n",
    "#         labels = train_df.label[train_df.Id == Id].tolist()[0].split(\"|\")\n",
    "# \n",
    "#         s, k, l, un_matched = form_labels(sentence=sentence, labels_list = labels)\n",
    "# \n",
    "#         if len(s) == 0:\n",
    "#             un_matched_reviews += 1\n",
    "#             un_mat.append(un_matched)\n",
    "#         else:\n",
    "#             sentences_.append(s)\n",
    "#             key_.append(k)\n",
    "#             labels_.append(l)\n",
    "#             Id_list_.append([Id]*len(l))\n",
    "# \n",
    "#     print(\"Total unmatched keywords:\", un_matched_reviews)\n",
    "#     sentences = [item for sublist in sentences_ for item in sublist]\n",
    "#     final_labels = [item for sublist in labels_ for item in sublist]\n",
    "#     keywords = [item for sublist in key_ for item in sublist]\n",
    "#     Id_list = [item for sublist in Id_list_ for item in sublist]\n",
    "# \n",
    "#     return sentences, final_labels, keywords, Id_list"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c759eac19a0bf2c7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test = df[df['document'] == 7]\n",
    "test"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "68961cd85005f6cd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_s = pd.Series(line.labels[0])\n",
    "test_s"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "87a69e30f4a9a588",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_s[test_s != 'O'].index"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ed82f19ceba0bd12",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.apply(lambda line: pd.Series(line['tokens'][0]).loc[pd.Series(line.labels[0])[pd.Series(line.labels[0]) != 'O'].index],axis = 1)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "70c6179bda828ca",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "d5bfcad9c5ddc3e5",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
